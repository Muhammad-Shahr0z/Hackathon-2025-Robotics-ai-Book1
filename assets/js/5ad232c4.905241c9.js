"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[853],{5060:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-2/index","title":"Module 2: The Digital Twin (Gazebo & Unity)","description":"Focus: Physics simulation and environment building.","source":"@site/docs/module-2/index.md","sourceDirName":"module-2","slug":"/module-2/","permalink":"/Hackathon-2025-Robotics-ai-Book1/docs/module-2/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Module 2: The Digital Twin (Gazebo & Unity)"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: ROS 2 Tools: RViz, Gazebo, and the CLI","permalink":"/Hackathon-2025-Robotics-ai-Book1/docs/module-1/chapter-3"},"next":{"title":"Chapter 4: Camera Systems and Image Processing in ROS 2","permalink":"/Hackathon-2025-Robotics-ai-Book1/docs/module-2/chapter-4"}}');var r=i(4848),s=i(8453);const o={title:"Module 2: The Digital Twin (Gazebo & Unity)"},a="Module 2: The Digital Twin (Gazebo & Unity)",l={},d=[{value:"Overview",id:"overview",level:2},{value:"What You Will Learn",id:"what-you-will-learn",level:2},{value:"Chapters",id:"chapters",level:2},{value:"Chapter 4: Camera Systems and Image Processing in ROS 2",id:"chapter-4-camera-systems-and-image-processing-in-ros-2",level:3},{value:"Chapter 5: Lidar and Depth Sensing: Building Point Clouds",id:"chapter-5-lidar-and-depth-sensing-building-point-clouds",level:3},{value:"Chapter 6: Sensor Fusion: Combining Data for Robust Perception",id:"chapter-6-sensor-fusion-combining-data-for-robust-perception",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Module Duration",id:"module-duration",level:2},{value:"Getting Started",id:"getting-started",level:2}];function c(e){const n={a:"a",br:"br",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"module-2-the-digital-twin-gazebo--unity",children:"Module 2: The Digital Twin (Gazebo & Unity)"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Focus"}),": Physics simulation and environment building."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"This module teaches you how to create digital twins of robots and environments. You'll learn to simulate physics, build realistic worlds, and integrate high-fidelity rendering for human-robot interaction studies."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"what-you-will-learn",children:"What You Will Learn"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Simulating physics, gravity, and collisions in Gazebo"}),"\n",(0,r.jsx)(n.li,{children:"High-fidelity rendering and human-robot interaction in Unity"}),"\n",(0,r.jsx)(n.li,{children:"Simulating sensors: LiDAR, Depth Cameras, and IMUs"}),"\n",(0,r.jsx)(n.li,{children:"Building realistic environments for robot testing"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"chapters",children:"Chapters"}),"\n",(0,r.jsx)(n.h3,{id:"chapter-4-camera-systems-and-image-processing-in-ros-2",children:"Chapter 4: Camera Systems and Image Processing in ROS 2"}),"\n",(0,r.jsx)(n.p,{children:"Process visual data from cameras to enable robot perception."}),"\n",(0,r.jsx)(n.h3,{id:"chapter-5-lidar-and-depth-sensing-building-point-clouds",children:"Chapter 5: Lidar and Depth Sensing: Building Point Clouds"}),"\n",(0,r.jsx)(n.p,{children:"Use Lidar and depth cameras to create 3D representations of the environment."}),"\n",(0,r.jsx)(n.h3,{id:"chapter-6-sensor-fusion-combining-data-for-robust-perception",children:"Chapter 6: Sensor Fusion: Combining Data for Robust Perception"}),"\n",(0,r.jsx)(n.p,{children:"Integrate multiple sensors to create a comprehensive understanding of the environment."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Completion of Module 1"}),"\n",(0,r.jsx)(n.li,{children:"Understanding of ROS 2 fundamentals"}),"\n",(0,r.jsx)(n.li,{children:"Basic knowledge of 3D graphics (helpful but not required)"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"module-duration",children:"Module Duration"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Estimated Time"}),": 2-3 weeks (Weeks 6-7 of the course)",(0,r.jsx)(n.br,{}),"\n",(0,r.jsx)(n.strong,{children:"Difficulty"}),": Intermediate"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,r.jsxs)(n.p,{children:["Begin with ",(0,r.jsx)(n.a,{href:"/Hackathon-2025-Robotics-ai-Book1/docs/module-2/chapter-4",children:"Chapter 4: Camera Systems and Image Processing"})," to learn about visual perception."]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var t=i(6540);const r={},s=t.createContext(r);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);