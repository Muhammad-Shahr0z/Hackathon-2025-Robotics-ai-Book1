"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[89],{2496:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/chapter-11","title":"Chapter 11: Vision-Language-Action (VLA) Models","description":"Description: Integrate large language models with robot control.","source":"@site/docs/module-4/chapter-11.md","sourceDirName":"module-4","slug":"/module-4/chapter-11","permalink":"/Hackathon-2025-Robotics-ai-Book1/docs/module-4/chapter-11","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Chapter 11: Vision-Language-Action (VLA) Models"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 10: NVIDIA Isaac Sim and Omniverse","permalink":"/Hackathon-2025-Robotics-ai-Book1/docs/module-4/chapter-10"},"next":{"title":"Chapter 12: Deploying to Real Hardware","permalink":"/Hackathon-2025-Robotics-ai-Book1/docs/module-4/chapter-12"}}');var a=o(4848),i=o(8453);const r={title:"Chapter 11: Vision-Language-Action (VLA) Models"},s="Chapter 11: Vision-Language-Action (VLA) Models",l={},c=[{value:"Topics",id:"topics",level:2},{value:"Hands-on",id:"hands-on",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-11-vision-language-action-vla-models",children:"Chapter 11: Vision-Language-Action (VLA) Models"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Description"}),": Integrate large language models with robot control."]}),"\n",(0,a.jsx)(n.h2,{id:"topics",children:"Topics"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Voice-to-action pipelines"}),"\n",(0,a.jsx)(n.li,{children:"LLM-based task planning"}),"\n",(0,a.jsx)(n.li,{children:"Natural language robot commands"}),"\n",(0,a.jsx)(n.li,{children:"Cognitive planning architectures"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"hands-on",children:"Hands-on"}),"\n",(0,a.jsx)(n.p,{children:"Command a robot using natural language."})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>s});var t=o(6540);const a={},i=t.createContext(a);function r(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);